{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Dataset for loading and sliding over time-series data.\"\"\"\n",
    "    def __init__(self, topo_array, debris_array):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with topography and debris arrays.\n",
    "\n",
    "        Args:\n",
    "        topo_array (numpy.ndarray): The topography 2D array.\n",
    "        debris_array (numpy.ndarray): The debris velocity or thickness 2D array.\n",
    "        \"\"\"\n",
    "        # Ensure that the input arrays are of type float32\n",
    "        self.topo_array = topo_array.astype('float32')\n",
    "        self.debris_array = debris_array.astype('float32')\n",
    "        # Combine the arrays along the last axis to create 3D dataset (time, height, width)\n",
    "        self.data = np.stack((self.topo_array, self.debris_array), axis=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of timesteps in the dataset.\"\"\"\n",
    "        return self.data.shape[0] - 1  # minus 1 because we need pairs (input, next)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get the (input, target) pair corresponding to the timestep idx.\"\"\"\n",
    "        return (self.data[idx], self.data[idx + 1])\n",
    "\n",
    "def create_dataloaders(topo_array, debris_array, batch_size=32, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create DataLoader instances for training and testing.\n",
    "\n",
    "    Args:\n",
    "    topo_array (numpy.ndarray): The topography 2D array.\n",
    "    debris_array (numpy.ndarray): The debris velocity or thickness 2D array.\n",
    "    batch_size (int): The batch size for DataLoader.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): The random state for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    DataLoader, DataLoader: The training and testing dataloaders.\n",
    "    \"\"\"\n",
    "    dataset = TimeSeriesDataset(topo_array, debris_array)\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(dataset)),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=5, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Train the neural network model.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model to train.\n",
    "    train_loader (DataLoader): DataLoader for the training data.\n",
    "    test_loader (DataLoader): DataLoader for the testing data.\n",
    "    epochs (int): The number of epochs to train for.\n",
    "    learning_rate (float): The learning rate for the optimizer.\n",
    "    \"\"\"\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Move the model to the GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print statistics\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        print(f'Test Loss: {test_loss / len(test_loader)}')\n",
    "\n",
    "# Example usage:\n",
    "# Assuming topo_array and debris_array are your preprocessed NumPy arrays\n",
    "input_shape = (2, topo_array.shape[1], topo_array.shape[2])  # (channels, height, width)\n",
    "model = SimpleNN(input_shape=input_shape)\n",
    "train_loader, test_loader = create_dataloaders(topo_array, debris_array)\n",
    "train_model(model, train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r\"data\\SKW01-SF01_CDF_150s_5mGrid_TS0\\RAW_DATA\"\n",
    "\n",
    "\n",
    "# Get the parent directory of dir_path\n",
    "parent_dir = os.path.dirname(os.path.normpath(dir_path))\n",
    "\n",
    "# Extract the folder name from the parent directory\n",
    "folder_name = os.path.basename(parent_dir)\n",
    "\n",
    "# Construct the file paths\n",
    "node_csv_path = os.path.join(dir_path, f\"{folder_name}_nodes.csv\")\n",
    "shell_csv_path = os.path.join(dir_path, f\"{folder_name}_shells.csv\")\n",
    "solid_csv_path = os.path.join(dir_path, f\"{folder_name}_solids.csv\")\n",
    "nodal_vels_csv_path = os.path.join(dir_path, f\"{folder_name}_nodal_velocities.csv\")\n",
    "solid_thickness_csv_path = os.path.join(dir_path, f\"{folder_name}_solid_thicknesses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xyz_round_converter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Define the CSV files and their respective parameters\u001b[39;00m\n\u001b[0;32m     19\u001b[0m csv_files \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodes\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m: node_csv_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconverters\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mxyz_round_converter\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_col\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNode_Label\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshells\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m: shell_csv_path},\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolids\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m: solid_csv_path},\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodal_vels\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m: nodal_vels_csv_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconverters\u001b[39m\u001b[38;5;124m'\u001b[39m: nodal_vels_converter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_col\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNode_Label\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolids_thick\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m'\u001b[39m: solid_thickness_csv_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconverters\u001b[39m\u001b[38;5;124m'\u001b[39m: solid_thick_converter}\n\u001b[0;32m     25\u001b[0m }\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Use ThreadPoolExecutor to read CSV files in parallel\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Submit tasks to the executor to read each CSV file in parallel\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xyz_round_converter' is not defined"
     ]
    }
   ],
   "source": [
    "xyz_round_converter = {col_name: partial(round_to_n_decimals, n=2) for col_name in ['Node_X', 'Node_Y', 'Node_Z']}\n",
    "\n",
    "def load_csv(file_path, converters=None, index_col=None):\n",
    "    \"\"\"\n",
    "    Load a CSV file into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the CSV file.\n",
    "    converters (dict): Dictionary of functions for converting values in certain columns, defaults to None.\n",
    "    index_col (str): Column to set as index, defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A pandas DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, converters=converters)\n",
    "    if index_col:\n",
    "        df.set_index(index_col, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Define the CSV files and their respective parameters\n",
    "csv_files = {\n",
    "    'nodes': {'file_path': node_csv_path, 'converters': xyz_round_converter, 'index_col': 'Node_Label'},\n",
    "    'shells': {'file_path': shell_csv_path},\n",
    "    'solids': {'file_path': solid_csv_path},\n",
    "    'nodal_vels': {'file_path': nodal_vels_csv_path, 'converters': nodal_vels_converter, 'index_col': 'Node_Label'},\n",
    "    'solids_thick': {'file_path': solid_thickness_csv_path, 'converters': solid_thick_converter}\n",
    "}\n",
    "\n",
    "# Use ThreadPoolExecutor to read CSV files in parallel\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Submit tasks to the executor to read each CSV file in parallel\n",
    "    future_to_csv = {executor.submit(load_csv, **params): name for name, params in csv_files.items()}\n",
    "    \n",
    "    # Collect the DataFrames as they are completed\n",
    "    for future in concurrent.futures.as_completed(future_to_csv):\n",
    "        name = future_to_csv[future]\n",
    "        try:\n",
    "            data_frame = future.result()\n",
    "            globals()[f\"{name}_df\"] = data_frame\n",
    "        except Exception as exc:\n",
    "            print(f'{name} generated an exception: {exc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdyna-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

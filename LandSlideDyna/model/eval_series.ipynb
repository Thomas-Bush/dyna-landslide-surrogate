{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# append the path of the parent directory\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# External library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# internal library imports\n",
    "\n",
    "# internal library imports\n",
    "from dataset import DebrisStateSeriesDataset\n",
    "from model import ConvLSTM, ConvLSTMComplex, UNetLSTM\n",
    "from train import TrainerSeries\n",
    "from util.setting_utils import set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for multiple libraries to ensure repeatability\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_losses(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Extract the subfolder name as the series name\n",
    "    series_name = os.path.basename(os.path.dirname(file_path))\n",
    "    return series_name, data['training_losses'], data['validation_losses']\n",
    "\n",
    "def plot_multiple_losses(files):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for file_path in files:\n",
    "        series_name, training_losses, validation_losses = read_losses(file_path)\n",
    "        epochs = range(1, len(training_losses) + 1)\n",
    "        \n",
    "        # plt.plot(epochs, training_losses, 'o-', label=f'{series_name} Training', linewidth=2)\n",
    "        plt.plot(epochs, validation_losses, 'o--', label=f'{series_name} Validation', linewidth=2)\n",
    "\n",
    "    plt.title('Training and Validation Losses for Multiple Models')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    '/home/tom/repos/dyna-landslide-surrogate/checkpoints/2024-04-20_base_model_comparison_l1Loss_batch32_CNN/losses_epoch_30.json',\n",
    "    '/home/tom/repos/dyna-landslide-surrogate/checkpoints/2024-04-20_base_model_comparison_l1Loss_batch32_SmallUNet/losses_epoch_30.json',\n",
    "    '/home/tom/repos/dyna-landslide-surrogate/checkpoints/2024-04-20_base_model_comparison_l1Loss_batch32_MedUNet/losses_epoch_30.json',\n",
    "    '/home/tom/repos/dyna-landslide-surrogate/checkpoints/2024-04-21_base_model_comparison_l1Loss_batch32_LargeUNet/losses_epoch_30.json'\n",
    "]\n",
    "\n",
    "plot_multiple_losses(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "root_dir = r'/home/tom/repos/dyna-landslide-surrogate/data_small'\n",
    "checkpoint_dir = r'/home/tom/repos/dyna-landslide-surrogate/checkpoints'\n",
    "batch_size = 64\n",
    "split_proportions = (0.7, 0.15, 0.15)\n",
    "epochs = 5\n",
    "seq_length = 5  # Specify the sequence length for the ConvLSTMUNet\n",
    "array_size = 256\n",
    "\n",
    "in_channels = 3  # Number of input channels (e.g., terrain, velocity, thickness)\n",
    "out_channels = 2  # Number of output channels (e.g., next velocity, next thickness)\n",
    "hidden_dim = 64  # Hidden dimension of the ConvLSTM\n",
    "kernel_size = (3, 3)  # Kernel size of the ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set up\n",
    "\n",
    "# Initialize dataset with scaling\n",
    "dataset = DebrisStateSeriesDataset(root_dir=root_dir, array_size=array_size, apply_scaling=True, sequence_length=seq_length)\n",
    "\n",
    "# Split dataset into train, validation, and test sets and create dataloaders\n",
    "train_loader, val_loader, test_loader = dataset.create_dataloaders(split_proportions, batch_size, random_state=42)\n",
    "\n",
    "# Dataset stats\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "print(f\"Train size: {len(train_loader.dataset)}, Validation size: {len(val_loader.dataset)}, Test size: {len(test_loader.dataset)}\")\n",
    "\n",
    "print(dataset.scaling_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of each architecture\n",
    "cnn = CNN()\n",
    "small_unet = UNet(in_channels=3, out_channels=2, features=[64, 128, 256])\n",
    "med_unet = UNet(in_channels=3, out_channels=2, features=[64, 128, 256, 512])\n",
    "large_unet = UNet(in_channels=3, out_channels=2, features=[64, 128, 256, 512, 1024])\n",
    "\n",
    "# Calculate the number of parameters for each architecture\n",
    "cnn_params = sum(p.numel() for p in cnn.parameters())\n",
    "small_unet_params = sum(p.numel() for p in small_unet.parameters())\n",
    "med_unet_params = sum(p.numel() for p in med_unet.parameters())\n",
    "large_unet_params = sum(p.numel() for p in large_unet.parameters())\n",
    "\n",
    "# Print the number of parameters for each architecture\n",
    "print(\"ComplexCNN parameters:\", cnn_params)\n",
    "print(\"SimpleUNet parameters:\", small_unet_params)\n",
    "print(\"LargeUNet parameters:\", med_unet_params)\n",
    "print(\"UNet parameters:\", large_unet_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "unet_lstm = UNetLSTM().to(device)\n",
    "\n",
    "# Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    # This will wrap the model for use with multiple GPUs\n",
    "    unet_lstm = nn.DataParallel(unet_lstm)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.L1Loss()\n",
    "# criterion = CustomDebrisLoss(loss_fn_zero=nn.SmoothL1Loss(), loss_fn_debris=nn.SmoothL1Loss(), debris_weight=0.66)\n",
    "optimizer = Adam(unet_lstm.parameters(), lr=1e-3)\n",
    "\n",
    "unet_lstm_trainer = TrainerSeries(unet_lstm, optimizer, criterion, device, model_name=\"cnn\", checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "unet_lstm_trainer.load_checkpoint(\"/home/tom/repos/dyna-landslide-surrogate/checkpoints/unet_lstm_test_int5_seq5_fulldataprune/model_epoch_5.pth\", train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_lstm_trainer.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet_lstm_trainer.plot_predictions(test_loader, num_predictions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_real_vs_inferred(real_states, inferred_states, n=5):\n",
    "    num_timesteps = len(real_states)\n",
    "    timesteps_to_plot = range(1, num_timesteps + 1, n)  # Start from 1 and go up to num_timesteps + 1\n",
    "\n",
    "    for timestep in timesteps_to_plot:\n",
    "        real_state = real_states[timestep]\n",
    "        inferred_state = inferred_states[timestep]\n",
    "\n",
    "        # Extract velocity and thickness from real and inferred states\n",
    "        real_velocity = real_state[1]\n",
    "        real_thickness = real_state[0]\n",
    "        inferred_velocity = inferred_state[1]\n",
    "        inferred_thickness = inferred_state[0]\n",
    "\n",
    "        # Calculate differences\n",
    "        velocity_diff = np.abs(real_velocity - inferred_velocity)\n",
    "        thickness_diff = np.abs(real_thickness - inferred_thickness)\n",
    "\n",
    "        plt.figure(figsize=(20, 8))\n",
    "\n",
    "        # Row 1: Real Velocity, Inferred Velocity, Velocity Difference\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.imshow(real_velocity, cmap='jet')\n",
    "        plt.title(f'Real Velocity (Timestep {timestep})')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.imshow(inferred_velocity, cmap='jet')\n",
    "        plt.title(f'Inferred Velocity (Timestep {timestep})')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.subplot(2, 3, 3)\n",
    "        plt.imshow(velocity_diff, cmap='jet')\n",
    "        plt.title(f'Velocity Difference (Timestep {timestep})')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "\n",
    "        # Row 2: Real Thickness, Inferred Thickness, Thickness Difference\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.imshow(real_thickness, cmap='jet')\n",
    "        plt.title(f'Real Thickness (Timestep {timestep})')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.imshow(inferred_thickness, cmap='jet')\n",
    "        plt.title(f'Inferred Thickness (Timestep {timestep})')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.imshow(thickness_diff, cmap='jet')\n",
    "        plt.title(f'Thickness Difference (Timestep {timestep})')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Stack all inferred arrays and all actual arrays\n",
    "    stacked_inferred_velocity = np.stack([state[1] for state in inferred_states.values()], axis=0)\n",
    "    stacked_inferred_thickness = np.stack([state[0] for state in inferred_states.values()], axis=0)\n",
    "    stacked_real_velocity = np.stack([state[1] for state in real_states.values()], axis=0)\n",
    "    stacked_real_thickness = np.stack([state[0] for state in real_states.values()], axis=0)\n",
    "\n",
    "    # Plot stacked arrays side by side\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(stacked_real_velocity.max(axis=0), cmap='jet')\n",
    "    plt.title('Stacked Real Velocity')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.imshow(stacked_inferred_velocity.max(axis=0), cmap='jet')\n",
    "    plt.title('Stacked Inferred Velocity')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.imshow(np.abs(stacked_real_velocity.max(axis=0) - stacked_inferred_velocity.max(axis=0)), cmap='jet')\n",
    "    plt.title('Stacked Velocity Difference')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.imshow(stacked_real_thickness.max(axis=0), cmap='jet')\n",
    "    plt.title('Stacked Real Thickness')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(stacked_inferred_thickness.max(axis=0), cmap='jet')\n",
    "    plt.title('Stacked Inferred Thickness')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(np.abs(stacked_real_thickness.max(axis=0) - stacked_inferred_thickness.max(axis=0)), cmap='jet')\n",
    "    plt.title('Stacked Thickness Difference')\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_real_states_dict(root_dir, model_number, start_state, num_timesteps, array_size):\n",
    "    real_states = {}\n",
    "    \n",
    "    model_dir = os.path.join(root_dir, str(model_number))\n",
    "    velocity_dir = os.path.join(model_dir, f'04_FinalProcessedData_{array_size}', 'velocity')\n",
    "    thickness_dir = os.path.join(model_dir, f'04_FinalProcessedData_{array_size}', 'thickness')\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        state_number = start_state + t\n",
    "        \n",
    "        velocity_file = os.path.join(velocity_dir, f'{model_number}_velocity_{state_number}.npy')\n",
    "        thickness_file = os.path.join(thickness_dir, f'{model_number}_thickness_{state_number}.npy')\n",
    "        \n",
    "        velocity = np.load(velocity_file)\n",
    "        thickness = np.load(thickness_file)\n",
    "        \n",
    "        real_states[t + 1] = np.stack((thickness, velocity), axis=0)  # Change the key to t + 1\n",
    "    \n",
    "    return real_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = '00194'\n",
    "\n",
    "\n",
    "# Create the initial input\n",
    "initial_input = unet_lstm_trainer.create_inference_input(root_dir, model_id, 10, 256)\n",
    "\n",
    "# Perform inference\n",
    "num_timesteps = 30\n",
    "inferred_states = unet_lstm_trainer.infer(initial_input, num_timesteps)\n",
    "\n",
    "# Create the real states dictionary\n",
    "real_states = create_real_states_dict(root_dir, model_id, num_timesteps+1, num_timesteps, 256)\n",
    "\n",
    "# Plot real vs inferred states\n",
    "plot_real_vs_inferred(real_states, inferred_states, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(initial_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_states[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, array in inferred_states.items():\n",
    "    min_value = np.min(array)\n",
    "    max_value = np.max(array)\n",
    "    print(f\"Min and max values for {key}: ({min_value}, {max_value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the initial input\n",
    "initial_input = med_unet_trainer.create_inference_input(root_dir, '00204', 2, 256)\n",
    "\n",
    "# Perform inference\n",
    "num_timesteps = 30\n",
    "inferred_states = med_unet_trainer.infer(initial_input, num_timesteps)\n",
    "\n",
    "# Create the real states dictionary\n",
    "real_states = create_real_states_dict(root_dir, '00204', 2 + 1, num_timesteps, 256)\n",
    "\n",
    "# Plot real vs inferred states\n",
    "plot_real_vs_inferred(real_states, inferred_states, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyna-landslide-surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

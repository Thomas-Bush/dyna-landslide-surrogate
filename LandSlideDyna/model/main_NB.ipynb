{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# append the path of the parent directory\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from model import SmallBasicUNet, MediumBasicUNet, MediumUNetPlus, CNNLSTM  # Replace with your actual model class name if different\n",
    "from dataset import DebrisFlowDataset, Augmentation  # Replace with your actual dataset class name if different\n",
    "from dataset import compute_channel_scaling_params, set_channel_scaling_params_to_dataset\n",
    "from train import Trainer\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "\n",
    "from viz import ArrayVisualizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed_value)  # NumPy\n",
    "    torch.manual_seed(seed_value)  # PyTorch\n",
    "    random.seed(seed_value)  # Python's built-in random module\n",
    "    \n",
    "    # minimise non-deterministic GPU behaviour\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Usage\n",
    "set_seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "SEQUENCE_LENGTH = 5\n",
    "BATCH_SIZE = 8\n",
    "LSTM_HIDDEN_SIZE = 256\n",
    "LSTM_LAYERS = 8\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "# # Load the complete dataset\n",
    "# full_dataset = DebrisFlowDataset(r'/home/tom/Downloads/data_small_temp', input_array_size=256, sequence_length=SEQUENCE_LENGTH)\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# train_size = int(0.7 * len(full_dataset))\n",
    "# val_size = int(0.15 * len(full_dataset))\n",
    "# test_size = len(full_dataset) - train_size - val_size\n",
    "# train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Create a DataLoader for the training dataset to compute scaling parameters\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split_by_model(full_dataset, train_frac=0.7, val_frac=0.15, test_frac=0.15):\n",
    "    # Ensure fractions sum to 1\n",
    "    assert np.isclose(train_frac + val_frac + test_frac, 1.0), \"The fractions must sum to 1.\"\n",
    "\n",
    "    # Extract the model IDs from the dataset\n",
    "    model_ids = [data_info[0] for data_info in full_dataset.data_info]\n",
    "    unique_model_ids = list(set(model_ids))\n",
    "\n",
    "    # Shuffle the unique model IDs\n",
    "    np.random.shuffle(unique_model_ids)\n",
    "\n",
    "    # Calculate the number of models for each set\n",
    "    num_models = len(unique_model_ids)\n",
    "    num_train = int(np.floor(train_frac * num_models))\n",
    "    num_val = int(np.floor(val_frac * num_models))\n",
    "\n",
    "    # The rest goes to the test set to account for any rounding issues\n",
    "    num_test = num_models - num_train - num_val\n",
    "\n",
    "    # Split the model IDs into train, val, and test sets based on the calculated numbers\n",
    "    train_model_ids = set(unique_model_ids[:num_train])\n",
    "    val_model_ids = set(unique_model_ids[num_train:num_train + num_val])\n",
    "    test_model_ids = set(unique_model_ids[num_train + num_val:])\n",
    "\n",
    "    # Verify that we have non-empty validation and test sets\n",
    "    assert len(val_model_ids) > 0, \"Validation set is empty. Adjust the fractions or dataset size.\"\n",
    "    assert len(test_model_ids) > 0, \"Test set is empty. Adjust the fractions or dataset size.\"\n",
    "\n",
    "    # Create indices for train, val, and test sets\n",
    "    train_indices = [i for i, id in enumerate(model_ids) if id in train_model_ids]\n",
    "    val_indices = [i for i, id in enumerate(model_ids) if id in val_model_ids]\n",
    "    test_indices = [i for i, id in enumerate(model_ids) if id in test_model_ids]\n",
    "\n",
    "    # Create subsets for train, val, and test\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "    test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Load the complete dataset\n",
    "# full_dataset = DebrisFlowDataset(r'/home/tom/Downloads/data_small_temp', input_array_size=256, sequence_length=SEQUENCE_LENGTH)\n",
    "full_dataset = DebrisFlowDataset(r'/home/tom/repos/dyna-landslide-surrogate/data', input_array_size=256, sequence_length=SEQUENCE_LENGTH)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = train_val_test_split_by_model(full_dataset, train_frac=0.7, val_frac=0.15)\n",
    "\n",
    "# Create a DataLoader for the training dataset to compute scaling parameters\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset.data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Access the file information in the full_dataset\n",
    "# def get_data_info(dataset, indices):\n",
    "#     # Retrieve the data info for the given indices\n",
    "#     return [dataset.data_info[idx][0] for idx in indices]\n",
    "\n",
    "\n",
    "# def collect_unique_filenames(data_info, indices):\n",
    "#     elevation_files = set()\n",
    "#     thickness_files = set()\n",
    "#     velocity_files = set()\n",
    "\n",
    "#     for idx in indices:\n",
    "#         _, data = data_info[idx]\n",
    "#         elevation_files.add(data['elevation_file'])\n",
    "#         thickness_files.update(data['thickness_files'])\n",
    "#         velocity_files.update(data['velocity_files'])\n",
    "\n",
    "#     return elevation_files, thickness_files, velocity_files\n",
    "\n",
    "\n",
    "# # Retrieve the data info for the training, validation, and test datasets\n",
    "# train_data_info = get_data_info(full_dataset, train_dataset.indices)\n",
    "# val_data_info = get_data_info(full_dataset, val_dataset.indices)\n",
    "# test_data_info = get_data_info(full_dataset, test_dataset.indices)\n",
    "\n",
    "# unique_elevation_files, unique_thickness_files, unique_velocity_files = collect_unique_filenames(full_dataset.data_info, train_dataset.indices)\n",
    "\n",
    "# # Print out debugging information for unique elevation files\n",
    "# print(f\"Number of unique elevation files: {len(unique_elevation_files)}\")\n",
    "# print(\"Examples of elevation files:\")\n",
    "# for filename in list(unique_elevation_files)[:5]:  # Print out the first 5 examples\n",
    "#     print(filename)\n",
    "\n",
    "# # Print out debugging information for unique thickness files\n",
    "# print(f\"\\nNumber of unique thickness files: {len(unique_thickness_files)}\")\n",
    "# print(\"Examples of thickness files:\")\n",
    "# for filename in list(unique_thickness_files)[:5]:  # Print out the first 5 examples\n",
    "#     print(filename)\n",
    "\n",
    "# # Print out debugging information for unique velocity files\n",
    "# print(f\"\\nNumber of unique velocity files: {len(unique_velocity_files)}\")\n",
    "# print(\"Examples of velocity files:\")\n",
    "# for filename in list(unique_velocity_files)[:5]:  # Print out the first 5 examples\n",
    "#     print(filename)\n",
    "\n",
    "# # # Now you can print or inspect the data info\n",
    "# # print('Train dataset data len:', len(train_data_info))\n",
    "# # print('Validation dataset data len:', len(val_data_info))\n",
    "# # print('Test dataset data len:', len(test_data_info))\n",
    "# # print('Train dataset data info:', train_data_info)\n",
    "# # print('Validation dataset data info:', val_data_info)\n",
    "# # print('Test dataset data info:', test_data_info)\n",
    "\n",
    "# # print(len(train_data_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert lists to sets for faster operation\n",
    "# train_data_set = set(train_data_info)\n",
    "# val_data_set = set(val_data_info)\n",
    "# test_data_set = set(test_data_info)\n",
    "\n",
    "# # Assert that there are no common elements between any two sets\n",
    "# assert train_data_set.isdisjoint(val_data_set), \"Training and validation sets have overlapping elements.\"\n",
    "# assert train_data_set.isdisjoint(test_data_set), \"Training and test sets have overlapping elements.\"\n",
    "# assert val_data_set.isdisjoint(test_data_set), \"Validation and test sets have overlapping elements.\"\n",
    "\n",
    "# print(\"All datasets are disjoint. No overlapping elements found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset.data_info[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize statistics\n",
    "# batch_sizes = []\n",
    "# shapes = {}\n",
    "# min_vals = {}\n",
    "# max_vals = {}\n",
    "# mean_vals = {}\n",
    "# var_vals = {}\n",
    "\n",
    "\n",
    "# # Iterate over the DataLoader\n",
    "# for i, (inputs, targets) in enumerate(train_loader):\n",
    "#     # Record batch size\n",
    "#     batch_sizes.append(inputs.size(0))\n",
    "    \n",
    "#     # Check shapes and record them\n",
    "#     for j, input_tensor in enumerate(inputs):\n",
    "#         if i == 0:  # Initialize stats dictionaries on the first batch\n",
    "#             shapes[j] = []\n",
    "#             min_vals[j] = []\n",
    "#             max_vals[j] = []\n",
    "#             mean_vals[j] = []\n",
    "#             var_vals[j] = []\n",
    "        \n",
    "#         shapes[j].append(input_tensor.shape)\n",
    "#         min_vals[j].append(input_tensor.min().item())\n",
    "#         max_vals[j].append(input_tensor.max().item())\n",
    "#         mean_vals[j].append(input_tensor.mean().item())\n",
    "#         var_vals[j].append(input_tensor.var().item())\n",
    "    \n",
    "\n",
    "\n",
    "# # Compute global statistics\n",
    "# total_batches = len(batch_sizes)\n",
    "# global_batch_size = sum(batch_sizes) / len(batch_sizes)\n",
    "# global_min_vals = {k: min(v) for k, v in min_vals.items()}\n",
    "# global_max_vals = {k: max(v) for k, v in max_vals.items()}\n",
    "# global_mean_vals = {k: sum(v) / len(v) for k, v in mean_vals.items()}\n",
    "# global_var_vals = {k: sum(v) / len(v) for k, v in var_vals.items()}\n",
    "\n",
    "# # Print statistics\n",
    "# print(f\"Total number of batches: {total_batches}\")\n",
    "# print(f\"Average batch size: {global_batch_size}\")\n",
    "# print(f\"Min values per channel: {global_min_vals}\")\n",
    "# print(f\"Max values per channel: {global_max_vals}\")\n",
    "# print(f\"Mean values per channel: {global_mean_vals}\")\n",
    "# print(f\"Variance values per channel: {global_var_vals}\")\n",
    "\n",
    "# # If you want to see individual batch stats:\n",
    "# print(f\"Batch sizes: {batch_sizes}\")\n",
    "# for channel, channel_shapes in shapes.items():\n",
    "#     print(f\"Shapes for channel {channel}: {channel_shapes}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the index of the batch you want to retrieve\n",
    "# desired_batch_index = 3  # Replace with the batch index you want to retrieve\n",
    "\n",
    "# # Variables to store the inputs and targets for the desired batch\n",
    "# desired_inputs = None\n",
    "# desired_targets = None\n",
    "\n",
    "# # Iterate over the DataLoader and retrieve the desired batch\n",
    "# for i, (inputs, targets) in enumerate(train_loader):\n",
    "#     if i == desired_batch_index:\n",
    "#         desired_inputs = inputs\n",
    "#         desired_targets = targets\n",
    "#         break  # Exit the loop after retrieving the desired batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired_targets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_input_channels_side_by_side(tensor, index, title_prefix):\n",
    "#     \"\"\"Plot channels 1 and 2 for a given sequence index from a 5D tensor side by side.\"\"\"\n",
    "#     fig, axs = plt.subplots(1, tensor.size(1), figsize=(15, 5))  # tensor.size(1) is the sequence length\n",
    "#     for seq_num in range(tensor.size(1)):  # Loop through the sequence length\n",
    "#         for channel in range(1, 3):  # Plot only channels 1 and 2\n",
    "#             channel_data = tensor[index, seq_num, channel, :, :].cpu().detach().numpy()\n",
    "#             axs[seq_num].imshow(channel_data, cmap='viridis')\n",
    "#             axs[seq_num].set_title(f'{title_prefix} - Seq {seq_num} - Channel {channel}')\n",
    "#             axs[seq_num].axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# def plot_target_channels_side_by_side(tensor, index, channel_titles):\n",
    "#     \"\"\"Plot the two target channels side by side.\"\"\"\n",
    "#     fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # 2 target channels\n",
    "#     for channel, title in enumerate(channel_titles):\n",
    "#         channel_data = tensor[index, :, :, channel].cpu().detach().numpy()\n",
    "#         axs[channel].imshow(channel_data, cmap='viridis')\n",
    "#         axs[channel].set_title(f'{title} - Data Item {index}')\n",
    "#         axs[channel].axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# # Set the index of the sequence you want to plot within the retrieved batch\n",
    "# sequence_index_to_plot = 0  # Replace with the sequence index you want to plot within the batch\n",
    "\n",
    "# # Plot the input channels for the specified sequence index within the retrieved batch\n",
    "# if desired_inputs is not None:\n",
    "#     plot_input_channels_side_by_side(desired_inputs, sequence_index_to_plot, title_prefix=\"Input\")\n",
    "\n",
    "# # Plot the target channels for the specified sequence index within the retrieved batch\n",
    "# if desired_targets is not None:\n",
    "#     plot_target_channels_side_by_side(desired_targets, sequence_index_to_plot, channel_titles=[\"Target Velocity\", \"Target Thickness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Total items in dataset: {len(train_loader.dataset)}\")\n",
    "# print(f\"Batch size: {train_loader.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute scaling parameters from the training DataLoader\n",
    "# median_vals, mad_vals = compute_channel_scaling_params(unique_elevation_files, unique_thickness_files, unique_velocity_files)\n",
    "\n",
    "# print(median_vals)\n",
    "\n",
    "# print(mad_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HybridMSELoss(nn.Module):\n",
    "    def __init__(self, non_zero_weight=0.8):\n",
    "        super(HybridMSELoss, self).__init__()\n",
    "        self.non_zero_weight = non_zero_weight\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Overall MSE\n",
    "        mse_loss = F.mse_loss(input, target, reduction='mean')\n",
    "        \n",
    "        # Masked MSE for non-zero elements\n",
    "        mask = target != 0\n",
    "        masked_input = input[mask]\n",
    "        masked_target = target[mask]\n",
    "        \n",
    "        if masked_target.nelement() == 0:  # Handle case with no non-zero elements\n",
    "            non_zero_mse_loss = 0\n",
    "        else:\n",
    "            non_zero_mse_loss = F.mse_loss(masked_input, masked_target, reduction='mean')\n",
    "        \n",
    "        # Combined loss\n",
    "        combined_loss = (self.non_zero_weight * non_zero_mse_loss +\n",
    "                         (1 - self.non_zero_weight) * mse_loss)\n",
    "        return combined_loss\n",
    "    \n",
    "class AsymmetricMSELoss(nn.Module):\n",
    "    def __init__(self, false_negative_weight=2.0, false_positive_weight=0.5):\n",
    "        super(AsymmetricMSELoss, self).__init__()\n",
    "        self.false_negative_weight = false_negative_weight\n",
    "        self.false_positive_weight = false_positive_weight\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Calculate the squared error\n",
    "        squared_error = (input - target) ** 2\n",
    "\n",
    "        # Create masks for false negatives and false positives\n",
    "        false_negatives = (input == 0) & (target != 0)\n",
    "        false_positives = (input != 0) & (target == 0)\n",
    "\n",
    "        # Apply weights to the errors\n",
    "        loss = squared_error\n",
    "        loss[false_negatives] *= self.false_negative_weight\n",
    "        loss[false_positives] *= self.false_positive_weight\n",
    "\n",
    "        # Calculate the final loss as the mean of the weighted errors\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n",
      "Epoch 1/100 - Train Loss: 17.1027, Validation Loss: 18.5181\n",
      "Epoch 2/100 - Train Loss: 15.7926, Validation Loss: 18.3781\n",
      "Epoch 3/100 - Train Loss: 15.3065, Validation Loss: 18.3237\n",
      "Epoch 4/100 - Train Loss: 14.9084, Validation Loss: 17.8427\n",
      "Epoch 5/100 - Train Loss: 14.4664, Validation Loss: 17.7634\n",
      "Epoch 6/100 - Train Loss: 14.1779, Validation Loss: 18.0550\n",
      "Epoch 7/100 - Train Loss: 13.9117, Validation Loss: 17.7162\n",
      "Epoch 8/100 - Train Loss: 13.7853, Validation Loss: 17.8330\n",
      "Epoch 9/100 - Train Loss: 13.5616, Validation Loss: 17.8448\n",
      "Epoch 10/100 - Train Loss: 13.4841, Validation Loss: 18.4818\n",
      "Epoch 11/100 - Train Loss: 13.3026, Validation Loss: 17.9241\n",
      "Epoch 12/100 - Train Loss: 13.1586, Validation Loss: 18.0550\n",
      "Epoch 13/100 - Train Loss: 13.1453, Validation Loss: 17.8848\n",
      "Epoch 14/100 - Train Loss: 12.9956, Validation Loss: 18.1634\n",
      "Epoch 15/100 - Train Loss: 12.7929, Validation Loss: 17.9054\n",
      "Epoch 16/100 - Train Loss: 12.7272, Validation Loss: 18.0273\n",
      "Epoch 17/100 - Train Loss: 12.5099, Validation Loss: 18.0274\n",
      "Epoch 18/100 - Train Loss: 12.3425, Validation Loss: 17.9538\n",
      "Epoch 19/100 - Train Loss: 12.0990, Validation Loss: 19.3627\n",
      "Epoch 20/100 - Train Loss: 12.0022, Validation Loss: 17.6374\n",
      "Epoch 21/100 - Train Loss: 11.7808, Validation Loss: 17.9507\n",
      "Epoch 22/100 - Train Loss: 11.6378, Validation Loss: 17.4948\n",
      "Epoch 23/100 - Train Loss: 11.4659, Validation Loss: 17.3638\n",
      "Epoch 24/100 - Train Loss: 11.3066, Validation Loss: 17.6794\n",
      "Epoch 25/100 - Train Loss: 11.1859, Validation Loss: 17.6485\n",
      "Epoch 26/100 - Train Loss: 11.0045, Validation Loss: 17.6042\n",
      "Epoch 27/100 - Train Loss: 10.8485, Validation Loss: 18.4023\n",
      "Epoch 28/100 - Train Loss: 10.7063, Validation Loss: 17.8879\n",
      "Epoch 29/100 - Train Loss: 10.6346, Validation Loss: 17.4558\n",
      "Epoch 30/100 - Train Loss: 10.5649, Validation Loss: 17.4822\n",
      "Epoch 31/100 - Train Loss: 10.3371, Validation Loss: 17.5427\n",
      "Epoch 32/100 - Train Loss: 10.2300, Validation Loss: 17.3550\n",
      "Epoch 33/100 - Train Loss: 10.0690, Validation Loss: 17.3399\n",
      "Epoch 34/100 - Train Loss: 10.0136, Validation Loss: 17.2012\n",
      "Epoch 35/100 - Train Loss: 9.8535, Validation Loss: 17.3851\n",
      "Epoch 36/100 - Train Loss: 9.7209, Validation Loss: 17.7101\n",
      "Epoch 37/100 - Train Loss: 9.6970, Validation Loss: 18.1730\n",
      "Epoch 38/100 - Train Loss: 9.5757, Validation Loss: 17.7752\n",
      "Epoch 39/100 - Train Loss: 9.4297, Validation Loss: 17.3823\n",
      "Epoch 40/100 - Train Loss: 9.3206, Validation Loss: 17.7729\n",
      "Epoch 41/100 - Train Loss: 9.2547, Validation Loss: 17.3461\n",
      "Epoch 42/100 - Train Loss: 9.2226, Validation Loss: 17.0708\n",
      "Epoch 43/100 - Train Loss: 9.0504, Validation Loss: 17.3328\n",
      "Epoch 44/100 - Train Loss: 9.0581, Validation Loss: 17.5112\n",
      "Epoch 45/100 - Train Loss: 8.9426, Validation Loss: 17.5953\n",
      "Epoch 46/100 - Train Loss: 8.9099, Validation Loss: 17.7444\n",
      "Epoch 47/100 - Train Loss: 8.8325, Validation Loss: 17.3957\n",
      "Epoch 48/100 - Train Loss: 8.7649, Validation Loss: 17.9106\n",
      "Epoch 49/100 - Train Loss: 8.6719, Validation Loss: 17.2640\n",
      "Epoch 50/100 - Train Loss: 8.6062, Validation Loss: 17.7255\n",
      "Epoch 51/100 - Train Loss: 8.5114, Validation Loss: 17.5025\n",
      "Epoch 52/100 - Train Loss: 8.5213, Validation Loss: 17.9604\n",
      "Epoch 53/100 - Train Loss: 8.3918, Validation Loss: 18.5512\n",
      "Epoch 54/100 - Train Loss: 8.3407, Validation Loss: 17.1115\n",
      "Epoch 55/100 - Train Loss: 8.3111, Validation Loss: 17.7562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(cnn_lstm_model, criterion, optimizer, device)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Training and validation\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Testing\u001b[39;00m\n\u001b[1;32m     41\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(test_loader)\n",
      "File \u001b[0;32m~/repos/dyna-landslide-surrogate/LandSlideDyna/model/train.py:25\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 25\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Apply augmentation if it is provided\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmentation\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda/lib/python3.12/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/repos/dyna-landslide-surrogate/LandSlideDyna/model/dataset.py:118\u001b[0m, in \u001b[0;36mDebrisFlowDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    115\u001b[0m velocity_files \u001b[38;5;241m=\u001b[39m sequence_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvelocity_files\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Load thickness and velocity sequences\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m thickness_sequence \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tf \u001b[38;5;129;01min\u001b[39;00m thickness_files[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length]]\n\u001b[1;32m    119\u001b[0m velocity_sequence \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mload(vf) \u001b[38;5;28;01mfor\u001b[39;00m vf \u001b[38;5;129;01min\u001b[39;00m velocity_files[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length]]\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Pad sequences if necessary (this code assumes sequences always have the desired length)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda/lib/python3.12/site-packages/numpy/lib/npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda/lib/python3.12/site-packages/numpy/lib/format.py:784\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    782\u001b[0m version \u001b[38;5;241m=\u001b[39m read_magic(fp)\n\u001b[1;32m    783\u001b[0m _check_version(version)\n\u001b[0;32m--> 784\u001b[0m shape, fortran_order, dtype \u001b[38;5;241m=\u001b[39m \u001b[43m_read_array_header\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    787\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda/lib/python3.12/site-packages/numpy/lib/format.py:645\u001b[0m, in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version, max_header_size)\u001b[0m\n\u001b[1;32m    642\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeader is not a dictionary: \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(d))\n\u001b[0;32m--> 645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m EXPECTED_KEYS \u001b[38;5;241m!=\u001b[39m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    646\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(d\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    647\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeader does not contain the correct keys: \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply the scaling parameters to the original dataset\n",
    "# train, val, test are type \"subset\" when using random_split (which just stores a list of idx)\n",
    "# set_channel_scaling_params_to_dataset(full_dataset, median_vals, mad_vals)\n",
    "\n",
    "# Now you can create DataLoaders for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# Model, criterion, and optimizer\n",
    "unet = MediumUNetPlus(in_channels=3, out_channels=2)\n",
    "cnn_lstm_model = CNNLSTM(unet=unet, lstm_hidden_size=LSTM_HIDDEN_SIZE, lstm_layers=LSTM_LAYERS)\n",
    "# criterion = torch.nn.MSELoss()  \n",
    "criterion = HybridMSELoss(non_zero_weight=0.75)\n",
    "#criterion = AsymmetricMSELoss(false_negative_weight=10.0, false_positive_weight=1.0)\n",
    "optimizer = torch.optim.Adam(cnn_lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# If you have multiple GPUs and want to use them, wrap your model with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "    # This will split your data automatically and send job orders to multiple GPUs\n",
    "    cnn_lstm_model = torch.nn.DataParallel(cnn_lstm_model)\n",
    "\n",
    "# Make sure to call .to(device) to move your model to the default device\n",
    "cnn_lstm_model.to('cuda')  # Assuming that you are using CUDA\n",
    "\n",
    "\n",
    "# Instantiate augmentation\n",
    "# TODO - CHECK THIS IS WORKING\n",
    "augmentation = Augmentation()\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(cnn_lstm_model, criterion, optimizer, device)\n",
    "\n",
    "# Training and validation\n",
    "trainer.train(train_loader, val_loader, NUM_EPOCHS)\n",
    "\n",
    "# Testing\n",
    "test_loss = trainer.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Testing\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 57\u001b[0m     test_loss, predicted, target \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mmodel)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[0;32m~/repos/dyna-landslide-surrogate/LandSlideDyna/model/train.py:109\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, test_loader, return_indices)\u001b[0m\n\u001b[1;32m    107\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    108\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[0;32m--> 109\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    111\u001b[0m all_outputs\u001b[38;5;241m.\u001b[39mappend(outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mHybridMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     15\u001b[0m masked_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[mask]\n\u001b[1;32m     16\u001b[0m masked_target \u001b[38;5;241m=\u001b[39m target[mask]\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmasked_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnelement\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Handle case with no non-zero elements\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     non_zero_mse_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For MINMAX SCALED\n",
    "# def inverse_transform(data, min_val, max_val):\n",
    "#     \"\"\"\n",
    "#     Inverse transform the data from the scaled range back to the original range.\n",
    "    \n",
    "#     Args:\n",
    "#         data (np.ndarray): The data to inverse transform.\n",
    "#         min_val (float): The minimum value of the original range.\n",
    "#         max_val (float): The maximum value of the original range.\n",
    "    \n",
    "#     Returns:\n",
    "#         np.ndarray: The inverse transformed data.\n",
    "#     \"\"\"\n",
    "#     return data * (max_val - min_val) + min_val\n",
    "\n",
    "def inverse_transform(data, median_val, iqr_val):\n",
    "    \"\"\"\n",
    "    Inverse transform the data from the scaled range back to the original range using median and IQR.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): The data to inverse transform.\n",
    "        median_val (float): The median value used for scaling.\n",
    "        iqr_val (float): The IQR value used for scaling.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The inverse transformed data.\n",
    "    \"\"\"\n",
    "    return (data * iqr_val) + median_val\n",
    "\n",
    "def plot_array(array, ax, title, label, cmap='viridis', vmin=None, vmax=None):\n",
    "    \"\"\"\n",
    "    Plot a 2D array as an image with an additional label.\n",
    "    \n",
    "    Args:\n",
    "        array (np.ndarray): The 2D array to plot.\n",
    "        ax (matplotlib.axes.Axes): The matplotlib axes to plot on.\n",
    "        title (str): The title of the plot.\n",
    "        label (str): The additional label to display on the plot.\n",
    "        cmap (str): The colormap to use (default: 'viridis').\n",
    "        vmin (float): The minimum value for the colorbar (default: None).\n",
    "        vmax (float): The maximum value for the colorbar (default: None).\n",
    "    \"\"\"\n",
    "    im = ax.imshow(array, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(f\"{title}\\n{label}\")\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "\n",
    "indices = [10,20,30,40,50,60]\n",
    "\n",
    "for i in indices:\n",
    "    # Testing\n",
    "    print(i)\n",
    "    test_loss, predicted, target = trainer.test(test_loader, return_indices=i)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "    model_name = type(trainer.model).__name__\n",
    "    model_state_id = id(trainer.model.state_dict())\n",
    "\n",
    "    plot_label = f\"Model: {model_name}, State ID: {model_state_id}\"\n",
    "\n",
    "    #  predicted and target values without inverse\n",
    "    predicted_thickness_inverse = predicted[..., 0]\n",
    "    target_thickness_inverse = target[..., 0]\n",
    "    predicted_velocity_inverse = predicted[..., 1]\n",
    "    target_velocity_inverse = target[..., 1]\n",
    "    \n",
    "    # Plot the predicted thickness, target thickness, and their difference\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    plot_array(predicted_thickness_inverse, ax=axes[0], title='Predicted Thickness', label=plot_label, vmin=0, vmax=7)\n",
    "    plot_array(target_thickness_inverse, ax=axes[1], title='Target Thickness', label=plot_label)\n",
    "    plot_array(predicted_thickness_inverse - target_thickness_inverse, ax=axes[2], title='Thickness Difference', label=plot_label, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "\n",
    "    # Plot the predicted velocity, target velocity, and their difference\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    plot_array(predicted_velocity_inverse, ax=axes[0], title='Predicted Velocity', label=plot_label)\n",
    "    plot_array(target_velocity_inverse, ax=axes[1], title='Target Velocity', label=plot_label)\n",
    "    plot_array(predicted_velocity_inverse - target_velocity_inverse, ax=axes[2], title='Velocity Difference', label=plot_label, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # # Inverse transform the predicted and target values using median and IQR\n",
    "    # predicted_thickness_inverse = inverse_transform(predicted[..., 0], median_vals['thickness'], iqr_vals['thickness'])\n",
    "    # target_thickness_inverse = inverse_transform(target[..., 0], median_vals['thickness'], iqr_vals['thickness'])\n",
    "    # predicted_velocity_inverse = inverse_transform(predicted[..., 1], median_vals['velocity'], iqr_vals['velocity'])\n",
    "    # target_velocity_inverse = inverse_transform(target[..., 1], median_vals['velocity'], iqr_vals['velocity'])\n",
    "\n",
    "\n",
    "    # # Plot the predicted thickness, target thickness, and their difference\n",
    "    # fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    # plot_array(predicted_thickness_inverse, ax=axes[0], title='Predicted Thickness', label=plot_label, vmin=0, vmax=7)\n",
    "    # plot_array(target_thickness_inverse, ax=axes[1], title='Target Thickness', label=plot_label)\n",
    "    # plot_array(predicted_thickness_inverse - target_thickness_inverse, ax=axes[2], title='Thickness Difference', label=plot_label, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # # Plot the predicted velocity, target velocity, and their difference\n",
    "    # fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    # plot_array(predicted_velocity_inverse, ax=axes[0], title='Predicted Velocity', label=plot_label)\n",
    "    # plot_array(target_velocity_inverse, ax=axes[1], title='Target Velocity', label=plot_label)\n",
    "    # plot_array(predicted_velocity_inverse - target_velocity_inverse, ax=axes[2], title='Velocity Difference', label=plot_label, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyna-landslide-surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

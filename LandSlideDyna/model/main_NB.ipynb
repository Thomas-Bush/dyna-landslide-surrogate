{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# append the path of the parent directory\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from model import SmallBasicUNet, BasicUNet, BasicUNetPLUS, CNNLSTM  # Replace with your actual model class name if different\n",
    "from dataset import DebrisFlowDataset, Augmentation  # Replace with your actual dataset class name if different\n",
    "from dataset import compute_channel_scaling_params, set_channel_scaling_params_to_dataset\n",
    "from train import Trainer\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from viz import ArrayVisualizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = ArrayVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed_value)  # NumPy\n",
    "    torch.manual_seed(seed_value)  # PyTorch\n",
    "    random.seed(seed_value)  # Python's built-in random module\n",
    "    \n",
    "    # minimise non-deterministic GPU behaviour\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Usage\n",
    "set_seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQUENCE_LENGTH = 5\n",
    "# BATCH_SIZE = 32\n",
    "# LSTM_HIDDEN_SIZE = 64\n",
    "# LSTM_LAYERS = 2\n",
    "# NUM_EPOCHS = 10\n",
    "\n",
    "\n",
    "# # Load the complete dataset\n",
    "# full_dataset = DebrisFlowDataset(r'C:\\Users\\thomas.bush\\repos\\dyna-landslide-surrogate\\data_small', sequence_length=SEQUENCE_LENGTH)\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# train_size = int(0.7 * len(full_dataset))\n",
    "# val_size = int(0.15 * len(full_dataset))\n",
    "# test_size = len(full_dataset) - train_size - val_size\n",
    "# train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Create a DataLoader for the training dataset to compute scaling parameters\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# # Compute scaling parameters from the training DataLoader\n",
    "# min_vals, max_vals = compute_channel_scaling_params(train_loader)\n",
    "\n",
    "# # Apply the scaling parameters to the original dataset\n",
    "# # train, val, test are type \"subset\" when using random_split (which just stores a list of idx)\n",
    "# set_channel_scaling_params_to_dataset(full_dataset, min_vals, max_vals)\n",
    "\n",
    "# # Now you can create DataLoaders for training, validation, and testing\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING CELLS\n",
    "\n",
    "Next few cells do some checking. Commented out for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def investigate_dataloader(dataloader, show_sample_data=True):\n",
    "#     \"\"\"Investigate a DataLoader by checking various aspects of the loaded data.\n",
    "\n",
    "#     Args:\n",
    "#         dataloader (torch.utils.data.DataLoader): The DataLoader to investigate.\n",
    "#         show_sample_data (bool): Flag to show a sample of the data.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     for i, data in enumerate(dataloader):\n",
    "#         # Assuming a (data, target) structure for each batch\n",
    "#         inputs, targets = data\n",
    "\n",
    "#         print(f\"Batch {i + 1}:\")\n",
    "\n",
    "#         # Check data shapes\n",
    "#         print(f\"  Input batch shape: {inputs.shape}\")\n",
    "#         print(f\"  Target batch shape: {targets.shape}\")\n",
    "\n",
    "#         # Check data types\n",
    "#         print(f\"  Input batch dtype: {inputs.dtype}\")\n",
    "#         print(f\"  Target batch dtype: {targets.dtype}\")\n",
    "\n",
    "#         # Check for correct batch size\n",
    "#         print(f\"  Batch size: {len(inputs)}\")\n",
    "\n",
    "#         # Optionally, print a sample data (for the first batch only if show_sample_data is True)\n",
    "#         if show_sample_data and i == 0:\n",
    "#             print(f\"  Input sample data:\\n{inputs[0]}\")\n",
    "#             print(f\"  Target sample data:\\n{targets[0]}\")\n",
    "\n",
    "#         # Stop after the first batch if we only want to show a sample\n",
    "#         if show_sample_data:\n",
    "#             break\n",
    "\n",
    "# def check_data_range(dataloader):\n",
    "#     \"\"\"Check if all tensors in the DataLoader have values between 0 and 1.\n",
    "\n",
    "#     Args:\n",
    "#         dataloader (torch.utils.data.DataLoader): The DataLoader to check.\n",
    "\n",
    "#     Returns:\n",
    "#         bool: True if all values are within the range [0, 1], False otherwise.\n",
    "#     \"\"\"\n",
    "#     for i, data in enumerate(dataloader):\n",
    "#         # Assuming a (data, target) structure for each batch\n",
    "#         inputs, targets = data\n",
    "        \n",
    "#         # Check the range for inputs and targets\n",
    "#         if inputs.min() < 0 or inputs.max() > 1 or targets.min() < 0 or targets.max() > 1:\n",
    "#             return False\n",
    "\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_data_in_range = check_data_range(test_loader)\n",
    "# print(f\"All tensors are in the range [0, 1]: {is_data_in_range}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset.data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dataset.__getitem__(1)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET CHECKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet = BasicUNetPLUS(in_channels=3, out_channels=3)\n",
    "\n",
    "# # Move the model to the device (GPU if available)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# unet.to(device)\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# unet.eval()\n",
    "\n",
    "# # Let's take a batch of data from your train_loader\n",
    "# # Assuming that the train_loader has been defined and is iterable\n",
    "# inputs, targets = next(iter(train_loader))\n",
    "\n",
    "# # Process a single batch, ignoring the sequence length for simplicity\n",
    "# # Here, we take the first time step of the sequence for demonstration purposes\n",
    "# input_batch = inputs[:, 0, :, :, :].to(device)  # Shape: [batch_size, channels, height, width]\n",
    "\n",
    "# # Forward pass to get the output from the model\n",
    "# with torch.no_grad():  # Disable gradient computation for inference\n",
    "#     output_batch = unet(input_batch)\n",
    "\n",
    "# # Move output back to CPU for further operations\n",
    "# output_batch = output_batch.cpu()\n",
    "\n",
    "# # output_batch now contains the U-Net predictions for the batch\n",
    "# print(f'Output batch shape: {output_batch.shape}')\n",
    "\n",
    "# # Optionally, convert the output tensor to a numpy array if you want to visualize or process it further\n",
    "# output_batch_np = output_batch.numpy()\n",
    "\n",
    "# # Now you can visualize or further process the output_batch_np as needed\n",
    "\n",
    "# viz.plot_array(output_batch[2][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an instance of your UNet model\n",
    "# unet = SmallBasicUNet(in_channels=3, out_channels=2)  # Adjust out_channels if necessary\n",
    "\n",
    "# # Instantiate the CNNLSTMModel using the UNet model\n",
    "# cnn_lstm_model = CNNLSTM(unet=unet, lstm_hidden_size=LSTM_HIDDEN_SIZE, lstm_layers=LSTM_LAYERS)\n",
    "\n",
    "# # Define a loss function and an optimizer\n",
    "# criterion = torch.nn.MSELoss()  # Replace with the loss function appropriate for your task\n",
    "# optimizer = torch.optim.Adam(cnn_lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# # Move the model to the GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# cnn_lstm_model.to(device)\n",
    "\n",
    "# # Lists for storing loss values\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     cnn_lstm_model.train()\n",
    "#     train_loss = 0.0\n",
    "#     for input_batch, target_batch in train_loader:\n",
    "#         # Move tensors to the appropriate device\n",
    "#         input_batch = input_batch.to(device)\n",
    "#         target_batch = target_batch.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         output_batch = cnn_lstm_model(input_batch)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(output_batch, target_batch)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Accumulate the training loss\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#     # Average training loss for the epoch\n",
    "#     train_loss /= len(train_loader)\n",
    "#     train_losses.append(train_loss)\n",
    "#     print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "#     # Validation loop\n",
    "#     cnn_lstm_model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for input_batch, target_batch in val_loader:\n",
    "#             # Move tensors to the appropriate device\n",
    "#             input_batch = input_batch.to(device)\n",
    "#             target_batch = target_batch.to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             output_batch = cnn_lstm_model(input_batch)\n",
    "\n",
    "#             # Compute loss\n",
    "#             loss = criterion(output_batch, target_batch)\n",
    "\n",
    "#             # Accumulate the validation loss\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#     # Average validation loss for the epoch\n",
    "#     val_loss /= len(val_loader)\n",
    "#     val_losses.append(val_loss)\n",
    "#     print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# # Save the model (optional, you may want to save the model periodically or after training)\n",
    "# torch.save(cnn_lstm_model.state_dict(), \"cnn_lstm_model.pth\")\n",
    "\n",
    "# # Plotting the training and validation loss\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(range(NUM_EPOCHS), train_losses, label='Training Loss')\n",
    "# plt.plot(range(NUM_EPOCHS), val_losses, label='Validation Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# cnn_lstm_model.eval()\n",
    "\n",
    "# # Initialize the test loss\n",
    "# test_loss = 0.0\n",
    "\n",
    "# # Test loop\n",
    "# with torch.no_grad():\n",
    "#     for input_batch, target_batch in test_loader:\n",
    "#         # Move tensors to the appropriate device\n",
    "#         input_batch = input_batch.to(device)\n",
    "#         target_batch = target_batch.to(device)\n",
    "\n",
    "#         # Forward pass to get the output from the model\n",
    "#         output_batch = cnn_lstm_model(input_batch)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(output_batch, target_batch)\n",
    "#         test_loss += loss.item()\n",
    "\n",
    "# # Calculate average test loss\n",
    "# test_loss /= len(test_loader)\n",
    "# print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN.PY MORE FORMAL CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.0103, Validation Loss: 0.0108\n",
      "Epoch 2/50 - Train Loss: 0.0108, Validation Loss: 0.0083\n",
      "Epoch 3/50 - Train Loss: 0.0083, Validation Loss: 0.0071\n",
      "Epoch 4/50 - Train Loss: 0.0071, Validation Loss: 0.0067\n",
      "Epoch 5/50 - Train Loss: 0.0067, Validation Loss: 0.0058\n",
      "Epoch 6/50 - Train Loss: 0.0058, Validation Loss: 0.0054\n",
      "Epoch 7/50 - Train Loss: 0.0054, Validation Loss: 0.0050\n",
      "Epoch 8/50 - Train Loss: 0.0050, Validation Loss: 0.0046\n",
      "Epoch 9/50 - Train Loss: 0.0046, Validation Loss: 0.0042\n",
      "Epoch 10/50 - Train Loss: 0.0042, Validation Loss: 0.0038\n",
      "Epoch 11/50 - Train Loss: 0.0038, Validation Loss: 0.0035\n",
      "Epoch 12/50 - Train Loss: 0.0035, Validation Loss: 0.0032\n",
      "Epoch 13/50 - Train Loss: 0.0032, Validation Loss: 0.0030\n",
      "Epoch 14/50 - Train Loss: 0.0030, Validation Loss: 0.0027\n",
      "Epoch 15/50 - Train Loss: 0.0027, Validation Loss: 0.0025\n",
      "Epoch 16/50 - Train Loss: 0.0025, Validation Loss: 0.0023\n",
      "Epoch 17/50 - Train Loss: 0.0023, Validation Loss: 0.0021\n",
      "Epoch 18/50 - Train Loss: 0.0021, Validation Loss: 0.0019\n",
      "Epoch 19/50 - Train Loss: 0.0019, Validation Loss: 0.0018\n",
      "Epoch 20/50 - Train Loss: 0.0018, Validation Loss: 0.0016\n",
      "Epoch 21/50 - Train Loss: 0.0016, Validation Loss: 0.0015\n",
      "Epoch 22/50 - Train Loss: 0.0015, Validation Loss: 0.0013\n",
      "Epoch 23/50 - Train Loss: 0.0013, Validation Loss: 0.0012\n",
      "Epoch 24/50 - Train Loss: 0.0012, Validation Loss: 0.0011\n",
      "Epoch 25/50 - Train Loss: 0.0011, Validation Loss: 0.0010\n",
      "Epoch 26/50 - Train Loss: 0.0010, Validation Loss: 0.0009\n",
      "Epoch 27/50 - Train Loss: 0.0009, Validation Loss: 0.0008\n",
      "Epoch 28/50 - Train Loss: 0.0008, Validation Loss: 0.0007\n",
      "Epoch 29/50 - Train Loss: 0.0007, Validation Loss: 0.0007\n",
      "Epoch 30/50 - Train Loss: 0.0007, Validation Loss: 0.0006\n",
      "Epoch 31/50 - Train Loss: 0.0006, Validation Loss: 0.0005\n",
      "Epoch 32/50 - Train Loss: 0.0005, Validation Loss: 0.0005\n",
      "Epoch 33/50 - Train Loss: 0.0005, Validation Loss: 0.0004\n",
      "Epoch 34/50 - Train Loss: 0.0004, Validation Loss: 0.0004\n",
      "Epoch 35/50 - Train Loss: 0.0004, Validation Loss: 0.0004\n",
      "Epoch 36/50 - Train Loss: 0.0004, Validation Loss: 0.0003\n",
      "Epoch 37/50 - Train Loss: 0.0003, Validation Loss: 0.0003\n",
      "Epoch 38/50 - Train Loss: 0.0003, Validation Loss: 0.0003\n",
      "Epoch 39/50 - Train Loss: 0.0003, Validation Loss: 0.0002\n",
      "Epoch 40/50 - Train Loss: 0.0002, Validation Loss: 0.0002\n",
      "Epoch 41/50 - Train Loss: 0.0002, Validation Loss: 0.0002\n",
      "Epoch 42/50 - Train Loss: 0.0002, Validation Loss: 0.0002\n",
      "Epoch 43/50 - Train Loss: 0.0002, Validation Loss: 0.0001\n",
      "Epoch 44/50 - Train Loss: 0.0001, Validation Loss: 0.0001\n",
      "Epoch 45/50 - Train Loss: 0.0001, Validation Loss: 0.0001\n",
      "Epoch 46/50 - Train Loss: 0.0001, Validation Loss: 0.0001\n",
      "Epoch 47/50 - Train Loss: 0.0001, Validation Loss: 0.0001\n",
      "Epoch 48/50 - Train Loss: 0.0001, Validation Loss: 0.0001\n",
      "Epoch 49/50 - Train Loss: 0.0001, Validation Loss: 0.0001\n",
      "Epoch 50/50 - Train Loss: 0.0001, Validation Loss: 0.0001\n",
      "Test Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "SEQUENCE_LENGTH = 5\n",
    "BATCH_SIZE = 32\n",
    "LSTM_HIDDEN_SIZE = 64\n",
    "LSTM_LAYERS = 2\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the complete dataset\n",
    "full_dataset = DebrisFlowDataset(r'C:\\Users\\thomas.bush\\repos\\dyna-landslide-surrogate\\data_small', sequence_length=SEQUENCE_LENGTH)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create a DataLoader for the training dataset to compute scaling parameters\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Compute scaling parameters from the training DataLoader\n",
    "min_vals, max_vals = compute_channel_scaling_params(train_loader)\n",
    "\n",
    "# Apply the scaling parameters to the original dataset\n",
    "# train, val, test are type \"subset\" when using random_split (which just stores a list of idx)\n",
    "set_channel_scaling_params_to_dataset(full_dataset, min_vals, max_vals)\n",
    "\n",
    "# Now you can create DataLoaders for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# Model, criterion, and optimizer\n",
    "unet = SmallBasicUNet(in_channels=3, out_channels=2)\n",
    "cnn_lstm_model = CNNLSTM(unet=unet, lstm_hidden_size=LSTM_HIDDEN_SIZE, lstm_layers=LSTM_LAYERS)\n",
    "criterion = torch.nn.MSELoss()  # Replace with the loss function appropriate for your task\n",
    "optimizer = torch.optim.Adam(cnn_lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Instantiate augmentation\n",
    "\n",
    "augmentation = Augmentation()\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(cnn_lstm_model, criterion, optimizer, device)\n",
    "\n",
    "# Training and validation\n",
    "trainer.train(train_loader, val_loader, NUM_EPOCHS)\n",
    "\n",
    "# Testing\n",
    "test_loss = trainer.test(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset data info: ['00007', '00400', '00295', '00009', '00006', '00003', '00004', '00072', '00010']\n",
      "Validation dataset data info: ['00184']\n",
      "Test dataset data info: ['00008', '00005', '00002']\n"
     ]
    }
   ],
   "source": [
    "# Access the file information in the full_dataset\n",
    "def get_data_info(dataset, indices):\n",
    "    # Retrieve the data info for the given indices\n",
    "    return [dataset.data_info[idx][0] for idx in indices]\n",
    "\n",
    "# Retrieve the data info for the training, validation, and test datasets\n",
    "train_data_info = get_data_info(full_dataset, train_dataset.indices)\n",
    "val_data_info = get_data_info(full_dataset, val_dataset.indices)\n",
    "test_data_info = get_data_info(full_dataset, test_dataset.indices)\n",
    "\n",
    "# Now you can print or inspect the data info\n",
    "print('Train dataset data info:', train_data_info)\n",
    "print('Validation dataset data info:', val_data_info)\n",
    "print('Test dataset data info:', test_data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE BELOW WORKED!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters (these would be defined based on your project requirements)\n",
    "# batch_size = 9\n",
    "# lstm_hidden_size = 64\n",
    "# lstm_layers = 2\n",
    "# num_epochs = 10\n",
    "\n",
    "# # Create an instance of your UNet model\n",
    "# unet = SmallBasicUNet(in_channels=3, out_channels=3) \n",
    "\n",
    "# # Instantiate the CNNLSTMModel using the UNet model\n",
    "# cnn_lstm_model = CNNLSTM(unet=unet, lstm_hidden_size=lstm_hidden_size, lstm_layers=lstm_layers)\n",
    "\n",
    "# # Define a loss function and an optimizer\n",
    "# criterion = torch.nn.MSELoss()  # Replace with the loss function appropriate for your task\n",
    "# optimizer = torch.optim.Adam(cnn_lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# # Move the model to the GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# cnn_lstm_model.to(device)\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     cnn_lstm_model.train()\n",
    "#     for input_batch, target_batch in train_loader:\n",
    "#         # Move tensors to the appropriate device\n",
    "#         input_batch = input_batch.to(device)\n",
    "#         target_batch = target_batch.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         output_batch = cnn_lstm_model(input_batch)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(output_batch, target_batch)\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Log the loss (You might want to print or log the loss value to monitor training)\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "#     # Validation loop (if necessary)\n",
    "#     cnn_lstm_model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Validation code goes here\n",
    "#         pass\n",
    "\n",
    "#     # Save the model (optional, you may want to save the model periodically or after training)\n",
    "#     torch.save(cnn_lstm_model.state_dict(), \"cnn_lstm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyna-landslide-surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

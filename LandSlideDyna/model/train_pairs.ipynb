{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# append the path of the parent directory\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# External library imports\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "\n",
    "# internal library imports\n",
    "from dataset import DebrisStatePairsDataset\n",
    "from model import CNN, UNet\n",
    "from train import TrainerPairs, CustomDebrisLoss\n",
    "from util.setting_utils import set_seed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for multiple libraries to ensure repeatability\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "root_dir = r'/home/tom/repos/dyna-landslide-surrogate/data'\n",
    "checkpoint_dir = r'/home/tom/repos/dyna-landslide-surrogate/checkpoints'\n",
    "batch_size = 32\n",
    "split_proportions = (0.7, 0.15, 0.15)\n",
    "epochs = 50\n",
    "\n",
    "in_channels = 3  # Number of input channels (e.g., terrain, velocity, thickness)\n",
    "out_channels = 2  # Number of output channels (e.g., next velocity, next thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 54190\n",
      "Train size: 38302, Validation size: 7788, Test size: 8100\n"
     ]
    }
   ],
   "source": [
    "# Data set up\n",
    "\n",
    "# Initialize dataset with scaling\n",
    "dataset = DebrisStatePairsDataset(root_dir, array_size=256, apply_scaling=True, timestep_interval=5)\n",
    "\n",
    "# Split dataset into train, validation, and test sets and create dataloaders\n",
    "train_loader, val_loader, test_loader = dataset.create_dataloaders(split_proportions, batch_size, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Dataset stats\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "print(f\"Train size: {len(train_loader.dataset)}, Validation size: {len(val_loader.dataset)}, Test size: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair_filenames = dataset.get_pair_filenames()\n",
    "\n",
    "# # Print the filenames for each pair\n",
    "# for i, filenames in enumerate(pair_filenames):\n",
    "#     print(f\"Pair {i+1}:\")\n",
    "#     print(\"Current velocity:\", filenames['current_velocity'])\n",
    "#     print(\"Next velocity:\", filenames['next_velocity'])\n",
    "#     print(\"Current thickness:\", filenames['current_thickness'])\n",
    "#     print(\"Next thickness:\", filenames['next_thickness'])\n",
    "#     print(\"Terrain:\", filenames['terrain'])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_min_max(dataset):\n",
    "#     min_elevation = np.inf\n",
    "#     max_elevation = -np.inf\n",
    "#     min_velocity = np.inf\n",
    "#     max_velocity = -np.inf\n",
    "#     min_thickness = np.inf\n",
    "#     max_thickness = -np.inf\n",
    "\n",
    "#     for cnn_input, _ in dataset:\n",
    "#         elevation = cnn_input[0, :, :].numpy()\n",
    "#         thickness = cnn_input[1, :, :].numpy()\n",
    "#         velocity = cnn_input[2, :, :].numpy()\n",
    "\n",
    "#         min_elevation = min(min_elevation, elevation.min())\n",
    "#         max_elevation = max(max_elevation, elevation.max())\n",
    "#         min_velocity = min(min_velocity, velocity.min())\n",
    "#         max_velocity = max(max_velocity, velocity.max())\n",
    "#         min_thickness = min(min_thickness, thickness.min())\n",
    "#         max_thickness = max(max_thickness, thickness.max())\n",
    "\n",
    "#     return min_elevation, max_elevation, min_velocity, max_velocity, min_thickness, max_thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the range of values in each dataset\n",
    "# train_min_max = compute_min_max(train_loader.dataset)\n",
    "# val_min_max = compute_min_max(val_loader.dataset)\n",
    "# test_min_max = compute_min_max(test_loader.dataset)\n",
    "\n",
    "# print(\"Train dataset range:\")\n",
    "# print(\"Elevation: [{:.2f}, {:.2f}]\".format(*train_min_max[:2]))\n",
    "# print(\"Velocity: [{:.2f}, {:.2f}]\".format(*train_min_max[2:4]))\n",
    "# print(\"Thickness: [{:.2f}, {:.2f}]\".format(*train_min_max[4:]))\n",
    "\n",
    "# print(\"\\nValidation dataset range:\")\n",
    "# print(\"Elevation: [{:.2f}, {:.2f}]\".format(*val_min_max[:2]))\n",
    "# print(\"Velocity: [{:.2f}, {:.2f}]\".format(*val_min_max[2:4]))\n",
    "# print(\"Thickness: [{:.2f}, {:.2f}]\".format(*val_min_max[4:]))\n",
    "\n",
    "# print(\"\\nTest dataset range:\")\n",
    "# print(\"Elevation: [{:.2f}, {:.2f}]\".format(*test_min_max[:2]))\n",
    "# print(\"Velocity: [{:.2f}, {:.2f}]\".format(*test_min_max[2:4]))\n",
    "# print(\"Thickness: [{:.2f}, {:.2f}]\".format(*test_min_max[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "date = today.strftime('%Y-%m-%d')\n",
    "experiment = \"base_model_comparison_l1Loss_batch32_timstep5_lr1e-4_l2reg_drop0-25\"\n",
    "\n",
    "models = [\n",
    "    {'model': CNN(dropout_rate=0.25), 'name': f'{date}_{experiment}_CNN'},\n",
    "    {'model': UNet(in_channels=in_channels, out_channels=out_channels, features=[64, 128, 256], dropout_rate=0.25), 'name': f'{date}_{experiment}_SmallUNet'}, \n",
    "    {'model': UNet(in_channels=in_channels, out_channels=out_channels, features=[64, 128, 256, 512], dropout_rate=0.25), 'name': f'{date}_{experiment}_MedUNet'}, \n",
    "    {'model': UNet(in_channels=in_channels, out_channels=out_channels, features=[64, 128, 256, 512, 1024], dropout_rate=0.25), 'name': f'{date}_{experiment}_LargeUNet'}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an instance of each architecture\n",
    "# cnn = CNN()\n",
    "# small_unet = UNet(in_channels=3, out_channels=2, features=[64, 128, 256])\n",
    "# med_unet = UNet(in_channels=3, out_channels=2, features=[64, 128, 256, 512])\n",
    "# large_unet = UNet(in_channels=3, out_channels=2, features=[64, 128, 256, 512, 1024])\n",
    "\n",
    "# # Calculate the number of parameters for each architecture\n",
    "# cnn_params = sum(p.numel() for p in cnn.parameters())\n",
    "# small_unet_params = sum(p.numel() for p in small_unet.parameters())\n",
    "# med_unet_params = sum(p.numel() for p in med_unet.parameters())\n",
    "# large_unet_params = sum(p.numel() for p in large_unet.parameters())\n",
    "\n",
    "# # Print the number of parameters for each architecture\n",
    "# print(\"ComplexCNN parameters:\", cnn_params)\n",
    "# print(\"SimpleUNet parameters:\", small_unet_params)\n",
    "# print(\"LargeUNet parameters:\", med_unet_params)\n",
    "# print(\"UNet parameters:\", large_unet_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda.\n",
      "Using 2 GPUs!\n",
      "Training 2024-04-24_base_model_comparison_l1Loss_batch32_timstep5_lr1e-4_l2reg_drop0-25_CNN...\n",
      "Epoch [1/50], Loss: 0.0144\n",
      "Validation Loss: 0.0009\n",
      "Epoch [2/50], Loss: 0.0007\n",
      "Validation Loss: 0.0005\n",
      "Epoch [3/50], Loss: 0.0006\n",
      "Validation Loss: 0.0005\n",
      "Epoch [4/50], Loss: 0.0005\n",
      "Validation Loss: 0.0005\n",
      "Epoch [5/50], Loss: 0.0005\n",
      "Validation Loss: 0.0005\n",
      "Model saved to /home/tom/repos/dyna-landslide-surrogate/checkpoints/2024-04-24_base_model_comparison_l1Loss_batch32_timstep5_lr1e-4_l2reg_drop0-25_CNN/model_epoch_5.pth\n",
      "Losses saved to /home/tom/repos/dyna-landslide-surrogate/checkpoints/2024-04-24_base_model_comparison_l1Loss_batch32_timstep5_lr1e-4_l2reg_drop0-25_CNN/losses_epoch_5.json\n",
      "Epoch [6/50], Loss: 0.0005\n",
      "Validation Loss: 0.0004\n",
      "Epoch [7/50], Loss: 0.0005\n",
      "Validation Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "# Set up CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on device: {device}.\")\n",
    "\n",
    "# Train each model\n",
    "for model_info in models:\n",
    "    \n",
    "    model = model_info['model']\n",
    "\n",
    "    # Check if multiple GPUs are available and wrap the model using nn.DataParallel\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        # This will wrap the model for use with multiple GPUs\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    # criterion = nn.MSELoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    # criterion = CustomDebrisLoss(loss_fn_zero=nn.SmoothL1Loss(), loss_fn_debris=nn.SmoothL1Loss(), debris_weight=0.66)\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    \n",
    "    # Initialize the trainer\n",
    "    model_name = model_info['name']\n",
    "    trainer = TrainerPairs(model, optimizer, criterion, device, model_name=model_name, checkpoint_dir=checkpoint_dir)\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"Training {model_name}...\")\n",
    "    trainer.train(train_loader, val_loader, epochs=epochs, checkpoint_interval=5)\n",
    "    \n",
    "    print(f\"Finished training {model_name}.\")\n",
    "\n",
    "    # Clean-up\n",
    "    del model\n",
    "    del optimizer\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()  # Clear memory cache\n",
    "    gc.collect()  # Collect garbage\n",
    "    \n",
    "    print(f\"GPU memory cleared after training {model_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model on the test set\n",
    "# print(\"Plotting losses...\")\n",
    "# trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model on the test set\n",
    "# print(\"Evaluating the model on the test set...\")\n",
    "# trainer.test(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.plot_predictions(test_loader, num_predictions=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyna-landslide-surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "# append the path of the parent directory\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import FileUtils\n",
    "from visualisation import ArrayVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods - these need to be added to the data_processing.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata_to_dataframe(base_dir, folder_names):\n",
    "        \"\"\"Read JSON files from specified folders into a Pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            base_dir (str): The base directory containing the folders.\n",
    "            folder_names (list): List of folder names containing the JSON files.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A Pandas DataFrame containing the data from the JSON files.\n",
    "        \"\"\"\n",
    "        data_list = []\n",
    "\n",
    "        # Iterate over the folder names\n",
    "        for folder_name in folder_names:\n",
    "            file_path = os.path.join(base_dir, folder_name, f\"{folder_name}_metadata.json\")\n",
    "\n",
    "            # Read the JSON file, ensuring it exists\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    # Remove the 'timesteps' key from the data\n",
    "                    data.pop('timesteps', None)\n",
    "                    data_list.append(data)\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "\n",
    "        # Convert the list of dictionaries to a DataFrame\n",
    "        return pd.DataFrame(data_list)\n",
    "\n",
    "def enhance_metadata(df):\n",
    "    \"\"\"Enhance the DataFrame by adding x_dimension, y_dimension, and z_dimension columns.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the metadata.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The enhanced DataFrame with new dimension columns.\n",
    "    \"\"\"\n",
    "    # Calculate dimensions based on the min and max values\n",
    "    df['x_dimension'] = df['max_x_value'] - df['min_x_value']\n",
    "    df['y_dimension'] = df['max_y_value'] - df['min_y_value']\n",
    "    df['z_dimension'] = df['max_z_value'] - df['min_z_value']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_largest_bounding_box(base_dir, model_ids):\n",
    "    \"\"\"Find the largest bounding box dimensions and state with the largest bounding box.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): The base directory where model data is stored.\n",
    "        model_ids (list): A list of model IDs to process.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the overall bounding box dimensions and the details of the state with the largest bounding box.\n",
    "    \"\"\"\n",
    "    largest_bbox = {\n",
    "        'min_x': float('inf'),\n",
    "        'min_y': float('inf'),\n",
    "        'max_x': -float('inf'),\n",
    "        'max_y': -float('inf'),\n",
    "    }\n",
    "    largest_state_bbox = {\n",
    "        'state_id': None,\n",
    "        'min_x': float('inf'),\n",
    "        'min_y': float('inf'),\n",
    "        'max_x': -float('inf'),\n",
    "        'max_y': -float('inf'),\n",
    "        'dim_x': 0,\n",
    "        'dim_y': 0\n",
    "    }\n",
    "\n",
    "    for model_id in model_ids:\n",
    "        states_dir = os.path.join(base_dir, model_id, 'states')\n",
    "\n",
    "        for state_file in os.listdir(states_dir):\n",
    "            state_path = os.path.join(states_dir, state_file)\n",
    "            state_data = np.load(state_path)\n",
    "\n",
    "            # Assuming the state data is a structured array with 'x', 'y', 'thickness', 'velocity' fields\n",
    "            active_indices = np.nonzero(state_data['thickness'] * state_data['velocity'])\n",
    "            if active_indices[0].size == 0:  # Skip if there's no active debris\n",
    "                continue\n",
    "\n",
    "            min_x, max_x = state_data['x'][active_indices].min(), state_data['x'][active_indices].max()\n",
    "            min_y, max_y = state_data['y'][active_indices].min(), state_data['y'][active_indices].max()\n",
    "\n",
    "            # Update overall bounding box\n",
    "            largest_bbox['min_x'] = min(largest_bbox['min_x'], min_x)\n",
    "            largest_bbox['min_y'] = min(largest_bbox['min_y'], min_y)\n",
    "            largest_bbox['max_x'] = max(largest_bbox['max_x'], max_x)\n",
    "            largest_bbox['max_y'] = max(largest_bbox['max_y'], max_y)\n",
    "\n",
    "            # Check if this state has the largest bounding box\n",
    "            state_dim_x = max_x - min_x\n",
    "            state_dim_y = max_y - min_y\n",
    "            if state_dim_x * state_dim_y > largest_state_bbox['dim_x'] * largest_state_bbox['dim_y']:\n",
    "                largest_state_bbox.update({\n",
    "                    'state_id': state_file,\n",
    "                    'min_x': min_x,\n",
    "                    'min_y': min_y,\n",
    "                    'max_x': max_x,\n",
    "                    'max_y': max_y,\n",
    "                    'dim_x': state_dim_x,\n",
    "                    'dim_y': state_dim_y\n",
    "                })\n",
    "\n",
    "    overall_dim_x = largest_bbox['max_x'] - largest_bbox['min_x']\n",
    "    overall_dim_y = largest_bbox['max_y'] - largest_bbox['min_y']\n",
    "\n",
    "    return (\n",
    "        (largest_bbox['min_x'], largest_bbox['max_x'], largest_bbox['min_y'], largest_bbox['max_y'], overall_dim_x, overall_dim_y),\n",
    "        largest_state_bbox\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_states(base_dir, model_ids):\n",
    "    \"\"\"Load arrays for each state and perform checks on them, including max velocity and thickness.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): The base directory containing the model folders.\n",
    "        model_ids (list): List of model IDs corresponding to subfolders.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with bounding box details and max velocity and thickness for all states.\n",
    "    \"\"\"\n",
    "    results_data = []\n",
    "\n",
    "    for model_id in model_ids:\n",
    "        overall_min_x = overall_max_x = None\n",
    "        overall_min_y = overall_max_y = None\n",
    "        largest_bounding_box_dimensions = (0, 0)\n",
    "        largest_bounding_box_state = None\n",
    "        largest_bounding_box_coords = None\n",
    "        max_velocity = 0\n",
    "        max_thickness = 0\n",
    "\n",
    "        thickness_dir = os.path.join(base_dir, model_id, 'thickness')\n",
    "        velocity_dir = os.path.join(base_dir, model_id, 'velocity')\n",
    "\n",
    "        # Load real-world x and y coordinates\n",
    "        x_values = np.load(os.path.join(thickness_dir, f\"{model_id}_thickness_x_values.npy\"))\n",
    "        y_values = np.load(os.path.join(thickness_dir, f\"{model_id}_thickness_y_values.npy\"))\n",
    "\n",
    "        state_no = 1\n",
    "        while True:\n",
    "            thickness_file = os.path.join(thickness_dir, f\"{model_id}_thickness_{state_no}.npy\")\n",
    "            velocity_file = os.path.join(velocity_dir, f\"{model_id}_velocity_{state_no}.npy\")\n",
    "\n",
    "            # Check if both thickness and velocity files exist\n",
    "            if not os.path.exists(thickness_file) or not os.path.exists(velocity_file):\n",
    "                break  # No more states to process\n",
    "\n",
    "            thickness_array = np.load(thickness_file)\n",
    "            velocity_array = np.load(velocity_file)\n",
    "\n",
    "            # Update max velocity and thickness\n",
    "            max_velocity = max(max_velocity, np.max(velocity_array))\n",
    "            max_thickness = max(max_thickness, np.max(thickness_array))\n",
    "\n",
    "            # Determine the min and max xy coordinates of the debris\n",
    "            non_zero_indices = np.where((thickness_array > 0) & (velocity_array > 0))\n",
    "            if non_zero_indices[0].size > 0:\n",
    "                min_x, max_x = x_values[non_zero_indices[1].min()], x_values[non_zero_indices[1].max()]\n",
    "                min_y, max_y = y_values[non_zero_indices[0].min()], y_values[non_zero_indices[0].max()]\n",
    "\n",
    "                # Update overall bounding box\n",
    "                overall_min_x = min(overall_min_x, min_x) if overall_min_x is not None else min_x\n",
    "                overall_max_x = max(overall_max_x, max_x) if overall_max_x is not None else max_x\n",
    "                overall_min_y = min(overall_min_y, min_y) if overall_min_y is not None else min_y\n",
    "                overall_max_y = max(overall_max_y, max_y) if overall_max_y is not None else max_y\n",
    "\n",
    "                # Update largest bounding box state\n",
    "                bounding_box_dimensions = (max_x - min_x, max_y - min_y)\n",
    "                if np.prod(bounding_box_dimensions) > np.prod(largest_bounding_box_dimensions):\n",
    "                    largest_bounding_box_dimensions = bounding_box_dimensions\n",
    "                    largest_bounding_box_state = state_no\n",
    "                    largest_bounding_box_coords = (min_x, max_x, min_y, max_y)\n",
    "\n",
    "            state_no += 1\n",
    "\n",
    "        # Log the results for the current model\n",
    "        if overall_min_x is not None and overall_min_y is not None:\n",
    "            results_data.append({\n",
    "                'model_id': model_id,\n",
    "                'overall_min_x': overall_min_x,\n",
    "                'overall_max_x': overall_max_x,\n",
    "                'overall_dim_x': overall_max_x - overall_min_x,\n",
    "                'overall_min_y': overall_min_y,\n",
    "                'overall_max_y': overall_max_y,\n",
    "                'overall_dim_y': overall_max_y - overall_min_y,\n",
    "                'largest_bounding_box_state': largest_bounding_box_state,\n",
    "                'largest_bounding_box_min_x': largest_bounding_box_coords[0],\n",
    "                'largest_bounding_box_max_x': largest_bounding_box_coords[1],\n",
    "                'largest_bounding_box_dim_x': largest_bounding_box_dimensions[0],\n",
    "                'largest_bounding_box_min_y': largest_bounding_box_coords[2],\n",
    "                'largest_bounding_box_max_y': largest_bounding_box_coords[3],\n",
    "                'largest_bounding_box_dim_y': largest_bounding_box_dimensions[1],\n",
    "                'max_velocity': max_velocity,\n",
    "                'max_thickness': max_thickness\n",
    "            })\n",
    "\n",
    "                # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'../data/processed'\n",
    "\n",
    "model_ids = FileUtils.get_subfolder_names(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = read_metadata_to_dataframe(data_dir, model_ids)\n",
    "\n",
    "metadata_df = enhance_metadata(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = load_and_analyze_states(data_dir, model_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
